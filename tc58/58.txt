爬取规则

本次爬虫最重要的是代码的规范性与严谨性

采用ip池规避爬取的风险,但是58的不规则参数依然没有参透,不清楚是否会影响爬取的效率

从入口处爬取 ----> 爬取所有城市的链接入口 ----> 存入本地 -----> set去重 -----> 储存到列表

从列表pop ---->  获取所有房源的链接入口 ----> 使用hash去重 --->存入当前城市的列表中, 从url获取城市的英文名称 --->

       按城市存入当前城市的列表中 ---->

从获取到的房源的链接入口 -----> 获取爬取的内容 ----> 去重 ----> 存入mongodb


# 去重原理  ----------redis去重基本完成 -------->如果还需要优化，就需要将redis存进hash映射，使用布隆过滤器

将每一个URl用哈希算法加密，判断当前抓取的url有没有被抓取过

将城市里的网址存入一个列表，每次从头拿出一个并且删除拿出的那个，抓取完里面的内容后在回到列表进行抓取

主要去重是在一个大的Redis里面。将所有爬取过的URL给封存进去

钓鱼岛的链接需要特殊处理


# mongodb存储图片使用gridfs

# 打印日志的习惯 -----loging -->很重要


######### 需要攻克 ---<图片存储> ----<打印日志>


# 这2天58就改了抓取规则, 真是草拟吗了